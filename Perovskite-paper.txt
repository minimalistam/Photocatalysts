Summary of Machine Learning Works on Perovskite Bandgap Prediction

1. Data Preparation and Cleaning
- The dataset was compiled from literature sources.
- Data cleaning involved normalization of material formulas and handling of missing values.
- A "Normalized" dataset was created to serve as the ground truth for feature engineering.

2. Feature Engineering (Physics-Informed)
- A comprehensive set of physics-based features was generated using `1-add_physics_features.py`.
- Features were computed for A, B, and X sites independently, including:
  - Ionic radii (Shannon radii for different coordination numbers).
  - Electronegativity, atomic mass, ionization energy, electron affinity.
  - Valence electrons and d-electron counts.
  - Entropy and variance of site properties (for mixed-ion sites).
- Derived structural and chemical descriptors were calculated:
  - Tolerance factor (Goldschmidt).
  - Octahedral factor.
  - Electronegativity differences (bond polarity).
  - Reduced mass and mass ratios.
- Synthesis parameters (temperature, time) were included where available.

3. Feature Selection Strategy
- Initial experiments with Pearson correlation analysis (removing features with >70% correlation) showed a degradation in model performance.
- Consequently, a decision was made to retain the full set of physics-informed features, allowing the CatBoost model to handle feature interactions and redundancy natively.
- This approach maximized the information available to the model.

4. Machine Learning Pipeline
- Model: CatBoost Regressor was chosen for its native handling of categorical variables and missing data.
- Validation Strategy: 5-fold GroupKFold cross-validation was used to prevent data leakage, ensuring that materials with the same formula (but different synthesis parameters) stay in the same fold.
- Preprocessing: 
  - Outliers in the target variable (Bandgap) were winsorized (0.5th to 99.5th percentile).
  - Categorical features were encoded.
  - Missing values were handled natively by the model (Min strategy).

5. Hyperparameter Tuning
- The CatBoost model was optimized using Optuna (Bayesian optimization) to minimize RMSE.
- Optimized parameters included learning rate, tree depth, L2 regularization, and bagging temperature.
- This tuning process further refined the model's ability to generalize.

6. Model Comparison
- To validate the choice of CatBoost, we compared its performance against other state-of-the-art gradient boosting libraries (XGBoost and LightGBM) using the same 5-fold GroupKFold validation.
- Results (Average across 5 folds):
  - **CatBoost (Tuned): R² ≈ 0.735, RMSE ≈ 0.328 eV**
  - LightGBM: R² ≈ 0.685, RMSE ≈ 0.357 eV
  - XGBoost: R² ≈ 0.616, RMSE ≈ 0.393 eV
- CatBoost significantly outperformed the alternatives, confirming its suitability for this dataset, likely due to its superior handling of categorical features and small datasets.

7. Model Performance
- The final tuned CatBoost model achieved:
  - R² Score: ~0.735
  - Root Mean Squared Error (RMSE): ~0.328 eV
  - Mean Absolute Error (MAE): ~0.205 eV

8. Feature Importance & Data Synergy Analysis
- **Physics vs. Experimental Synergy**: The feature importance analysis revealed a crucial insight into how the model bridges the gap between diverse experimental conditions and fundamental material properties.
  - **Dominance of Physics Features**: The top predictive features were fundamental physics-derived properties, specifically **mass ratio (BX)**, **electron affinity (X)**, and **polarizability ratio**. These features describe the "ideal" electronic structure of the perovskite, providing a robust baseline prediction that is invariant to experimental noise.
  - **Role of Experimental Data**: Synthesis parameters like `synthesis_temperature` and `synthesis_time` appeared in the top 20 features but were less dominant than the physics features. This suggests that while synthesis conditions do affect the bandgap (likely by influencing crystal quality or defect density), the primary driver is the fundamental chemistry.
  - **Handling Diversity**: By anchoring the prediction in physics-based features (which are consistent regardless of who synthesized the material), the model effectively "normalizes" the diverse experimental data. The experimental features then act as fine-tuning parameters, allowing the model to adjust for the specific conditions under which a sample was made. This synergy allows the model to learn universal trends from a heterogeneous dataset.

9. Output and Artifacts
- The pipeline produces a trained model (`catboost_final.cbm`), a feature manifest (`feature_manifest.json`), and detailed prediction reports.
- **Visualizations**: High-quality figures for publication have been generated in the `paper_figures` folder, including:
  - `parity_plot.png`: Predicted vs Experimental Bandgap.
  - `residual_plot.png`: Residual analysis.
  - `feature_importance.png`: Top 15 most important features.
  - `error_distribution.png`: Histogram of prediction errors.

..........
Key Insights for your Paper:

Physics-Informed Synergy: The analysis revealed a powerful synergy. The top features are fundamental physics properties (Mass Ratio, Electron Affinity, Polarizability). These provide a stable "anchor" for the model, describing the ideal material behavior.
Handling Diversity: Because the physics features are consistent regardless of the synthesis method, they allow the model to learn universal trends across your diverse dataset.
Experimental Fine-Tuning: Synthesis parameters (Temperature, Time) appear as important but secondary features. They act as "fine-tuners," allowing the model to adjust the prediction based on the specific experimental conditions (defects, crystal quality) without getting confused by the diversity.
